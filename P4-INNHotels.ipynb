# INN Hotels Project

## Context

A significant number of hotel bookings are called-off due to cancellations or no-shows. The typical reasons for cancellations include change of plans, scheduling conflicts, etc. This is often made easier by the option to do so free of charge or preferably at a low cost which is beneficial to hotel guests but it is a less desirable and possibly revenue-diminishing factor for hotels to deal with. Such losses are particularly high on last-minute cancellations.

The new technologies involving online booking channels have dramatically changed customers’ booking possibilities and behavior. This adds a further dimension to the challenge of how hotels handle cancellations, which are no longer limited to traditional booking and guest characteristics.

The cancellation of bookings impact a hotel on various fronts:
* Loss of resources (revenue) when the hotel cannot resell the room.
* Additional costs of distribution channels by increasing commissions or paying for publicity to help sell these rooms.
* Lowering prices last minute, so the hotel can resell a room, resulting in reducing the profit margin.
* Human resources to make arrangements for the guests.

## Objective
The increasing number of cancellations calls for a Machine Learning based solution that can help in predicting which booking is likely to be canceled. INN Hotels Group has a chain of hotels in Portugal, they are facing problems with the high number of booking cancellations and have reached out to your firm for data-driven solutions. You as a data scientist have to analyze the data provided to find which factors have a high influence on booking cancellations, build a predictive model that can predict which booking is going to be canceled in advance, and help in formulating profitable policies for cancellations and refunds.

## Data Description
The data contains the different attributes of customers' booking details. The detailed data dictionary is given below.


**Data Dictionary**

* Booking_ID: unique identifier of each booking
* no_of_adults: Number of adults
* no_of_children: Number of Children
* no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel
* no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel
* type_of_meal_plan: Type of meal plan booked by the customer:
    * Not Selected – No meal plan selected
    * Meal Plan 1 – Breakfast
    * Meal Plan 2 – Half board (breakfast and one other meal)
    * Meal Plan 3 – Full board (breakfast, lunch, and dinner)
* required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)
* room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.
* lead_time: Number of days between the date of booking and the arrival date
* arrival_year: Year of arrival date
* arrival_month: Month of arrival date
* arrival_date: Date of the month
* market_segment_type: Market segment designation.
* repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)
* no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking
* no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking
* avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)
* no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)
* booking_status: Flag indicating if the booking was canceled or not.
## Importing necessary libraries and data
import warnings

warnings.filterwarnings("ignore")
from statsmodels.tools.sm_exceptions import ConvergenceWarning

warnings.simplefilter("ignore", ConvergenceWarning)


import pandas as pd
import numpy as np


import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 200)
pd.set_option("display.float_format", lambda x: "%.5f" % x)


from sklearn.model_selection import train_test_split


import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from statsmodels.tools.tools import add_constant
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree


from sklearn.model_selection import GridSearchCV



from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,
    plot_confusion_matrix,
    precision_recall_curve,
    roc_curve,
    make_scorer,
)
## Data Overview
### Load Dataset
df = pd.read_csv('INNHotelsGroup.csv')

#copying the data to another data_1
data = df.copy()
##First 5 rows of the dataset
data.head()
##Last 5 rows of the dataset
data.tail()
data.shape
#### There are 36275 rows with 19 columns in the dataset
data.info()
* `Booking_ID`,`type_of_meal_plan`,`room_type_reserved`,`market_segment_type` and `booking_status` are the columns with object type while the rest of the columns are numeric.
### Check for the mssing values and dropping the duplicate Values 
#Checking for null values
data.isnull().sum()
#Checking for duplicate values
data.duplicated().sum()
#Check for unique values
data.nunique()
#### As shown above the `Booking_ID` has 36275 unique values which is the number of rows in dataset. So let's drop the booking_ID column
data.drop(["Booking_ID"], axis=1, inplace=True)
## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.
**Leading Questions**:

Q1. What are the busiest months in the hotel?

A1. The Busiest month is october as shown in the graph

Q2. Which market segment do most of the guests come from?

A2. Most of the guests come from Online market segment

Q3. Hotel rates are dynamic and change according to demand and customer demographics. What are the differences in room prices in different market segments?

A3.Mostly the prices are same across all the market segments with online market segment being slightly higher

Q4. What percentage of bookings are canceled?

A4. Almost 33% of the bookings are canceled

Q5. Repeating guests are the guests who stay in the hotel often and are important to brand equity. What percentage of repeating guests cancel?

A5. Only around 2% of the repeated guests cancel

Q6. Many guests have special requirements when booking a hotel room. Do these requirements affect booking cancellation?

A6. There are total of 5 max special requirements when booking a room. People with more than 3 special requests never cancel
### Statistical Summary of the data
data.describe().T
* The number of adults are ranging from 0 to 4 with a mean of approaximately 2 adults per booking
* The number of children are ranging from 0 to 10 per booking
* On an average the reservations are done during the weekdays with atleast 2 nights per reservation with 17 days max whereas on weekend the average nights are booked 1 nights per reservations
### Univariate Analysis
#defining a histogram function
def hist_box(data, feature, figsize=(15, 10), kde=True, bins=None):
    
    f2, (ax_hist2,ax_box2) = plt.subplots(
        nrows=2,  
        sharex=True,
        gridspec_kw={"height_ratios": (0.75, 0.25)},
        figsize=figsize,
    )  
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--" #Mean line-Green
    )  
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-" #Median line-Black
    )  
#Function to create labeled barplot
def label_barplot(data, feature, perc=True, n=None):
    total = len(data[feature])  
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 2, 6))
    else:
        plt.figure(figsize=(n + 2, 6))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n],
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  
        else:
            label = p.get_height()  

        x = p.get_x() + p.get_width() / 2  
        y = p.get_height()  

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        ) 

    plt.show()
#### Leadtime Observations
As shown below most of the reservations are with 0 leading time meaning the reservations are done at the last moment or directly at the hotel
hist_box(data,"lead_time")
#### Average Price per Room graphs
As shown below the average price for the room is 100 € hut there are a lot of outliers in the dataset for Average price of the room.
hist_box(data,"avg_price_per_room")
data[data["avg_price_per_room"] == 0]
data.loc[data["avg_price_per_room"] == 0, "market_segment_type"].value_counts()
Q1 = data["avg_price_per_room"].quantile(0.25)   #25th quantile
Q1
Q3 = data["avg_price_per_room"].quantile(0.75)   #75th quantile 
Q3
IQR = Q3 - Q1      #IQR
IQR
U_Whisker = Q3 + 1.5 * IQR # Upper Outlier
U_Whisker
#Assigning the value of upper whisker to the outlier
data.loc[data["avg_price_per_room"] >=300, "avg_price_per_room"] = U_Whisker
data.loc[data["avg_price_per_room"] == U_Whisker]
#### Observation on the number of the previous booking not cancellations
hist_box(data,"no_of_previous_bookings_not_canceled")
#### Observation on the number of the previous booking cancellations
hist_box(data,"no_of_previous_cancellations")
#### Observations on Number of Adults

label_barplot(data, "no_of_adults", perc = True)
#### Observations on Number of Adults
label_barplot(data, "no_of_children", perc = True)
#### Observations on Number of Week Nights
label_barplot(data, "no_of_week_nights")
#### Observations on Number of Weekend Nights
label_barplot(data, "no_of_weekend_nights")
#### Observations on Required Parking Space
label_barplot(data, "required_car_parking_space")
#### Observations on Type of Meal Plan
label_barplot(data, "type_of_meal_plan")
#### Observations on Room Type Reserved
label_barplot(data,"room_type_reserved")
#### Observations on Market Segment Type
label_barplot(data,"market_segment_type")
#### Observations on Number of Special Requests
label_barplot(data, "no_of_special_requests")
#### Observations on Booking Status
label_barplot(data, "booking_status")
#let's Change the booking status data type to interger by one hot encoding
data["booking_status"] = data["booking_status"].apply(lambda x:1 if x == "Canceled" else 0)
data.head()
#### Observations on Arrival Month
label_barplot(data, "arrival_month")
### Bivariate Analysis
#Let's create the Heatmap
cols_list = data.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(12, 7))
sns.heatmap(
    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
#### Observations
* A positive correlation exists between repeated guests and the number of previous bookings not cancelled.
* There is a slightly positive correlation between the lead time and booking status.
* There's also a slightly negative correlation between the arrival year and arrival month.
#Function to Create the Plot distribution wrt target

def dist_plot_wrt_target(data, predictor, target):

    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    target_uniq = data[target].unique()

    axs[0, 0].set_title("Distribution of target for target=" + str(target_uniq[0]))
    sns.histplot(
        data=data[data[target] == target_uniq[0]],
        x=predictor,
        kde=True,
        ax=axs[0, 0],
        color="teal",
        stat="density",
    )

    axs[0, 1].set_title("Distribution of target for target=" + str(target_uniq[1]))
    sns.histplot(
        data=data[data[target] == target_uniq[1]],
        x=predictor,
        kde=True,
        ax=axs[0, 1],
        color="orange",
        stat="density",
    )

    axs[1, 0].set_title("Boxplot w.r.t target")
    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette="gist_rainbow")

    axs[1, 1].set_title("Boxplot (without outliers) w.r.t target")
    sns.boxplot(
        data=data,
        x=target,
        y=predictor,
        ax=axs[1, 1],
        showfliers=False,
        palette="gist_rainbow",
    )

    plt.tight_layout()
    plt.show()
def stack_barplot(data, predictor, target):
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 5))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.show()
#### The price of a hotel room vary according to the demographics and demand, Let's see the barplot of Avg Price WRT market segment
plt.figure(figsize=(10, 6))
sns.boxplot(data=data, x="market_segment_type", y="avg_price_per_room", palette="gist_rainbow")
plt.show()
stack_barplot(data,"market_segment_type","booking_status")
#### Observations 
* Around 38% of the online guests cancelled
* Almost 30% of the offline guests cancelled
* Only 20% the aviation guests cancelled
* Almost 10% corporate guests cancelled
#### Let's look at the barplot of Special requirements WRT to cancellations
stack_barplot(data,"no_of_special_requests", "booking_status")
* As predicted 100% of the guests with 5 special requests did not cancel
* Almost 40% of the guests cancelled with zero special requests
* Around 20% of the guests cancelled with 1 special requests
#### Let's look at the relationship between special requests and avg price
sns.barplot(data=data,x="no_of_special_requests",y="avg_price_per_room",palette="gist_rainbow")
plt.show()
#### Looking at the positive correlations between booking status and Average price per room 
dist_plot_wrt_target(data, "avg_price_per_room", "booking_status")
#### Graph of the booking status and lead time
dist_plot_wrt_target(data, "lead_time", "booking_status")
#### Creating a new dataset of the customers who travelled with the families, because people usually travel in group
family = data[(data["no_of_children"] >=0) & (data["no_of_adults"] > 1)]
family
family["no_of_family_members"] = (family["no_of_adults"] + family["no_of_children"])
family
stack_barplot(family,"no_of_family_members","booking_status")
#### Let's look at the booking status of the customers who has reseved it for atleast a day 
stay = data [(data["no_of_week_nights"] > 0) & (data["no_of_weekend_nights"]>0)]
stay["total_days"] = (stay["no_of_week_nights"]+stay["no_of_weekend_nights"])
stay
#Lets plot the total days with the booking status
stack_barplot(stay,"total_days","booking_status")
#Let's plot the graph of repeated guest with the booking status
stack_barplot(data,"repeated_guest", "booking_status")
#Plotting the graph to see the busiest month at the motel 
month = data.groupby(["arrival_month"])["booking_status"].count()

month = pd.DataFrame({"Arrival Month" : list(month.index), "Guest":list(month.values)})

sns.barplot(data=month,x="Arrival Month",y="Guest")
plt.show()
#### Looking at the booking cancelled in each month
stack_barplot(data,"arrival_month","booking_status")
#Average Prices per arrival month
sns.lineplot(data=data, x="arrival_month",y="avg_price_per_room", drawstyle='steps-pre')
plt.show()
## Data Preprocessing

#### As we looked above there is no null values but there are outliers in the data
#lets plot the bar plot of the numeric data types
num_col = data.select_dtypes(include=np.number).columns.tolist()
num_col.remove("booking_status")

plt.figure(figsize = (15,15))
for i, variable in enumerate(num_col):
    plt.subplot(4 ,4, i+1)
    plt.boxplot(data[variable], whis=1.5)
    plt.tight_layout()
    plt.title(variable)

plt.show()
* Number of adults and avg price per room have lower and upper outliers
* All the other columns have upper outliers 
#looking at the average price per room and why it is zero for some values
data.loc[data["avg_price_per_room"] == 0, "market_segment_type"].value_counts()
#### Out of total booking of 545 with price of zero euros 354 were complementary and 191 was online which means the prices are legit
##As caculated above and we already substituted the outlier value with upper whisker
U_Whisker
hist_box(data,"avg_price_per_room")
## EDA

- It is a good idea to explore the data once again after manipulating it.
data.describe().T
#lets look at the value counts for the numeric values
col_list = data.select_dtypes(include=["int64"]).columns.tolist()
for i in col_list:
    print(f"value count for {i}")
    print(data[i].value_counts())
    print(f"To verify total counts {data[i].count()}")
    print("*" * 50)
data["no_of_adults"].unique()
data["no_of_children"].unique()
data["no_of_weekend_nights"].unique()
data["no_of_week_nights"].unique()
data["required_car_parking_space"].unique()
data["lead_time"].unique()
data["repeated_guest"].unique()
data["arrival_month"].unique()
data["arrival_year"].unique()
data["no_of_previous_cancellations"].unique()
data["no_of_previous_bookings_not_canceled"].unique()
data["avg_price_per_room"].unique()
data["no_of_special_requests"].unique()
data["booking_status"].unique()
#### Based on the unique values of the columns the outliers seem reasonable as no unusual valued found
## Model Building
### Logistic Regression Model
#### Data Preparation 


* Predicting which bookings will be canceled.
* encoding categorical features.
* Splitting the data into train and test 
X = data.drop(["booking_status"], axis=1)
Y = data["booking_status"]

X = sm.add_constant(X)

X = pd.get_dummies (X, drop_first=True)

#Splitting the data in Train and Test
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size = 0.30, random_state=1)
X
print("Shape of Training set : ", X_train.shape)
print("Shape of test set : ", X_test.shape)
print("Percentage of classes in training set:")
print(y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(y_test.value_counts(normalize=True))
* As shown above the data is split into 70% and 30% as Train and Test data respectively
logit = sm.Logit(y_train,X_train.astype(float))
lg = logit.fit(disp=False)
print(lg.summary())
* Coefficinets in the model indicate the change in dependent variable 
* `arrival_date`,`type_of_meal_plan_Meal Plan 3`,`no_of_previous_bookings_not_canceled`,`market_segment_type_Complementary` and `market_segment_type_Online` have p value > 0.05
* Unit increase in no_of_repeated_guest while other variables constant decreases the probability of cancellation by 2.34

## Model evaluation criteria
#### Model can make wrong predictions in two ways:

1. Predicting a guest booking will not cancel, but in reality, booking cancels (False Positive)
2. Predicting a guest will cancel, but in reality, booking does not cancel (False Negative)

#### Determine Importance:
Both cases are important:

* If we predict that a booking will not cancel and booking gets canceled, hotel loses revenue and has to bear additional costs of distribution channels, lowering room price

* If we predict that a booking will cancel and booking isn't canceled, hotel might be unable to provide satisfactory services to guests by not being adequately staffed or prepared to host. This might damage brand equity/guest satisfaction and lead to opportunity loss

#### How to reduce the losses?
* Since both scenarios are to be addressed equally, hotel would want F1 Score to be maximized
* Greater the F1 score, higher the chances of minimizing both False Negatives and False Positives
* F1_score = (2 * P * R)/(P + R)
#### Let's create functions to calculate the performance metrics and confsion matrix

* model_performance_classification_statsmodels function ( To check performance of models)
* confusion_matrix_statsmodels function (To plot confusion matrix)
# Defining a function to calculate the performance of the classification models
def model_performance_classification_statsmodels(model, predictors, target, threshold=0.5):

    # checking which probabilities are greater than threshold
    pred_temp = model.predict(predictors) > threshold
    # rounding off the above values to get classes
    pred = np.round(pred_temp)

    acc = accuracy_score(target, pred)  # computing Accuracy
    recall = recall_score(target, pred)  # computing Recall
    precision = precision_score(target, pred)  # computing Precision
    f1 = f1_score(target, pred)  #computing F1-score

    # creating a dataframe of metrics
    data_perf = pd.DataFrame({"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},index=[0])

    return data_perf
#Function to plot the confusion matrix of the classification model

def confusion_matrix_statsmodels(model, predictors, target, threshold=0.5):
    y_pred = model.predict(predictors) > threshold
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
confusion_matrix_statsmodels(lg,X_train,y_train)
print("Performance of the classification model")
model_performance_classification_statsmodels(lg, X_train, y_train)
* As shown in the performance of the classification model F1 score is 0.68 we need to maximize it
* The model needs to be checked for variable multicollinearity and needs to be removed to obtain reliable p values
### Multicollinearity Check
* To check for Multicollinearity we need to use VIF
* IF VIF is around 1, there is little or No Correlation between independent variables
* IF VIF > 5 There is moderate collinearity
* IF VIF => 10 there is high_multi Collinearity 
* Let's create a function to calculate the VIF
#Function to calculate VIF
def check_vif(predictors):
    vif = pd.DataFrame()
    vif["feature"] = predictors.columns

    # calculating VIF
    vif["VIF"] = [variance_inflation_factor(predictors.values, i)
        for i in range(len(predictors.columns))
    ]
    return vif
check_vif(X_train)
* Only some dummy variables show multicollinearity which can be ignored
### Dropping the variables with high p-values
* We have to drop the variables with high p-values one by one and repeat the steps until we get the variables with less than 0.05 p-values
* Creating a function to drop the variable with high p_values one by one
# Initial columns 
cols = X_train.columns.tolist()

# setting an initial max p-value
max_p_value = 1

while len(cols) > 0:
    # defining the train set
    x_train_aux = X_train[cols]

    # fitting the model
    model = sm.Logit(y_train, x_train_aux).fit(disp=False)

    # getting the p-values and the maximum p-value
    p_values = model.pvalues
    max_p_value = max(p_values)

    # name of the variable with maximum p-value
    feature_with_p_max = p_values.idxmax()

    if max_p_value > 0.05:
        cols.remove(feature_with_p_max)
    else:
        break

selected_features = cols
print(selected_features)
X_train1 = X_train[selected_features]
X_test1 = X_test[selected_features]
logit1 = sm.Logit(y_train, X_train1.astype(float)) ## logistic regression on X_train1 and y_train
lg1 = logit1.fit(disp=False) ## code to fit logistic regression
print(lg1.summary()) ## print summary of model
* We have treated for Multicollinearity, the model lg1 has dropped 6 variables with p-value>0.05
* All the categorical variables in the model lg1 had less the 0.05 p-values so lg1 is final model 



### Let's look at the model performance
print('lg1 Model Performance')
model_performance_classification_statsmodels(lg1, X_train1, y_train)
#Let's try the model on the test data
model_performance_classification_statsmodels(lg1, X_test1, y_test)
#### As we can see the model lg1 gives comparable values for train and test data, with F1-score of 0.68 we will try to improve it
### We will convert the coefficient to odd (probability max = 1)
* The Coefficients of logistic regression model are in terms of log(odds), to find the odds we have to take exponential of the coefficients.Therefore, odds = exp(b)
* The Percentage change in odds is given as odds = (exp(b) - 1) * 100
# converting coefficients to odds
odds = np.exp(lg1.params)

# finding the percentage change
perc_change_odds = (np.exp(lg1.params) - 1) * 100

# removing limit from number of columns to display
pd.set_option("display.max_columns", None)

# adding the odds to a dataframe
pd.DataFrame({"Odds": odds, "Change_odd%": perc_change_odds}, index=X_train1.columns)
### Checking model performance on training set
confusion_matrix_statsmodels(lg1, X_train1, y_train)
## Check performance to X_train1 and y_train
print("Training performance:")
log_reg_model_train_perf = model_performance_classification_statsmodels(lg1, X_train1, y_train)
log_reg_model_train_perf
### Let's look at ROC-AUC on Training set
logit_roc_auc_train = roc_auc_score(y_train, lg1.predict(X_train1))
fpr, tpr, thresholds = roc_curve(y_train, lg1.predict(X_train1))
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label="Logistic Regression (area = %0.2f)" % logit_roc_auc_train)
plt.plot([0, 1], [0, 1], "r--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.01])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver operating characteristic")
plt.legend(loc="lower right")
plt.show()
#### Model Performance improvement 
* Let's see if the recall score can be improved further, by changing the model threshold using AUC-ROC curve.
#### Optimal threshold using AUC-ROC curve
# Optimal threshold as per AUC-ROC curve
# The optimal cut off would be where tpr is high and fpr is low
fpr, tpr, thresholds = roc_curve(y_train, lg1.predict(X_train1))

optimal_idx = np.argmax(tpr - fpr)
optimal_threshold_auc_roc = thresholds[optimal_idx]
print(optimal_threshold_auc_roc)
# Creating confusion matrix
confusion_matrix_statsmodels(lg1, X_train1, y_train)
# Checking the model performance for this model
log_reg_model_train_perf_threshold_auc_roc = model_performance_classification_statsmodels(lg1, X_train1, y_train, threshold=optimal_threshold_auc_roc)
print("Training Model  performance:")
log_reg_model_train_perf_threshold_auc_roc
#### Let's see Precision-Recall curve to see if we can find a better threshold
y_scores = lg1.predict(X_train1)
prec, rec, tre = precision_recall_curve(y_train, y_scores,)


def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label="precision")
    plt.plot(thresholds, recalls[:-1], "g--", label="recall")
    plt.xlabel("Threshold")
    plt.legend(loc="upper left")
    plt.ylim([0, 1])


plt.figure(figsize=(10, 7))
plot_prec_recall_vs_tresh(prec, rec, tre)
plt.show()
#setting up the threshold
optimal_threshold_curve = 0.42
#### At the threshold of 0.42 we can get balanced recall and precision
#### Checking model performance on training set
#### Now,let's set the threshold to 0.42 and find out the model performance 
confusion_matrix_statsmodels(lg1, X_train1, y_train)
log_reg_model_train_perf_threshold_curve = model_performance_classification_statsmodels(lg1, X_train1, y_train, threshold=optimal_threshold_curve)
print("Training performance:")
log_reg_model_train_perf_threshold_curve
#### Recall has reduced to 0.70 and Precision increased to 0.69 but the F1 metrics is still the same
#### Model is still good 
#### Let's check the model on test data
Using model with default threshold 
# creating confusion matrix
confusion_matrix_statsmodels(lg1, X_test1, y_test) ## confusion matrix for X_test1 and y_test
## performance on X_test1 and y_test
log_reg_model_test_perf = model_performance_classification_statsmodels(lg1,X_test1,y_test)

print("Model Test performance:")
log_reg_model_test_perf
* F1-score-0.67
* Accuracy - 0.80
* Recall - 0.63
* Precision - 0.72
* Model is working well on the test data
#### ROC curve on the test data
logit_roc_auc_train = roc_auc_score(y_test, lg1.predict(X_test1))
fpr, tpr, thresholds = roc_curve(y_test, lg1.predict(X_test1))
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label="Logistic Regression (area = %0.2f)" % logit_roc_auc_train)
plt.plot([0, 1], [0, 1], "r--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.01])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver operating characteristic")
plt.legend(loc="lower right")
plt.show()
#### Using the Optimal threshold = 0.37 on the model with test data
#plotting confusion matrix for optimal threshold 0.37
confusion_matrix_statsmodels(lg1,X_test1,y_test) 
#checking the model performance
log_reg_model_test_perf_threshold_auc_roc = model_performance_classification_statsmodels(lg1, X_test1, y_test, threshold=optimal_threshold_auc_roc)

print("Test performance:")
log_reg_model_test_perf_threshold_auc_roc

#### Using the threshold = 0.42 on the model with test data
#creating the confusion matrix 
confusion_matrix_statsmodels(lg1, X_test1, y_test) 
#checking the model performance
log_reg_model_test_perf_threshold = model_performance_classification_statsmodels(lg1, X_test1, y_test, threshold=0.42)
print("Test performance:")
log_reg_model_test_perf_threshold

## Building a Logistic Regression model
#### Model performance evaluation
#training model performance comparison
models_train_comp_df = pd.concat([log_reg_model_train_perf.T,log_reg_model_train_perf_threshold_auc_roc.T,log_reg_model_train_perf_threshold_curve.T],
    axis=1)
models_train_comp_df.columns = [
    "Logistic Regression-default Threshold",
    "Logistic Regression-0.37 Threshold",
    "Logistic Regression-0.42 Threshold",
]

print("Training performance comparison:")
models_train_comp_df
# test performance comparison
models_test_comp_df = pd.concat(
 [
 log_reg_model_test_perf.T,
 log_reg_model_test_perf_threshold_auc_roc.T,
 log_reg_model_test_perf_threshold.T,
 ],
 axis=1,
)
models_test_comp_df.columns = [
 "Logistic Regression-default Threshold (0.5)",
"Logistic Regression-0.37 Threshold",
"Logistic Regression-0.42 Threshold",
]
print("Test set performance comparison:")
models_test_comp_df
## Final Model SUmmary 
* All three models are performing well on both the training and test data without overfitting
* Model with threshold value of 0.37 is giving best F1-score
## Building a Decision Tree model
### Data Preparation for modelling( Decision Tree)
* We want to predict which bookings will be canceled
* We need to encode categorical features
* We will split the data into train and test to be able to evaluate the model
X = data.drop(["booking_status"], axis=1)
Y = data["booking_status"]

X = pd.get_dummies(X, drop_first=True) ## create dummies for X

# Splitting data in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1) 
print("Shape of Training set : ", X_train.shape)
print("Shape of test set : ", X_test.shape)
print("Percentage of classes in training set:")
print(y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(y_test.value_counts(normalize=True))
### Function to calculcate performance metrics and confusion matrix
#function to check the performance of a classification model
def model_performance_classification_sklearn(model, predictors, target):
    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},
        index=[0],
    )

    return df_perf
#Function to plot confusion matrix
def confusion_matrix_sklearn(model, predictors, target):
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
#### Building Decision Tree
model = DecisionTreeClassifier(random_state=1)
model.fit(X_train,y_train) ## fit decision tree on train data
#### Check the model on training set
confusion_matrix_sklearn(model, X_train, y_train)
decision_tree_perf_train = model_performance_classification_sklearn(model, X_train, y_train)
decision_tree_perf_train
#### Check the model on testing set
confusion_matrix_sklearn(model, X_test, y_test)
decision_tree_perf_test = model_performance_classification_sklearn(model, X_test, y_test)
decision_tree_perf_test
print(decision_tree_perf_train);
print(decision_tree_perf_test);
* As shown in the comparison there is high discrepancy between the test and training data in terms of Accuracy, Recall,Precision and F1-Score
* This means model is overfitting
### Let's prune the tree
* Before pruning the tree lets check important features 
feature_names = list(X_train.columns)
importances = model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(6, 6))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()
### Pre-Pruning

* Using the Gridsearch hyperparameter to pre prune the tree
# Choosing the classifier 
estimator = DecisionTreeClassifier(random_state=1, class_weight="balanced")

# Grid of parameters to choose from
parameters = {
    "max_depth": np.arange(2, 7, 2),
    "max_leaf_nodes": [50, 75, 150, 250],
    "min_samples_split": [10, 30, 50, 70],
}

# Type of scoring used to compare parameter combinations
acc_scorer = make_scorer(f1_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
### After Pre-pruning let's see the performance of the training set
confusion_matrix_sklearn(estimator, X_train, y_train) 
decision_tree_tune_perf_train = model_performance_classification_sklearn(estimator, X_train, y_train) 
decision_tree_tune_perf_train
### After Pre-pruning let's see the performance of the testing set
confusion_matrix_sklearn(estimator, X_test, y_test) 
decision_tree_tune_perf_test = model_performance_classification_sklearn(estimator, X_test, y_test)
decision_tree_tune_perf_test
### As we can see after pre-pruning 
* The parameters between train and test data have comparable values
#### Visualizing the Decision Tree
plt.figure(figsize=(20, 10))
out = tree.plot_tree(
    estimator,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)

for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")
        arrow.set_linewidth(1)
plt.show()
#Printing the text report of decision tree
print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))
#### Decision Tree output understanding
* If the `leadtime is`<=151.5,`no_of_special_requests`<= 0.5,`no_of_weekights`<=0.5,`avg_room_price` >= 196.50, `market_segment_online`<=0.5,then the booking is likely to get cancelled
#lets see the important features after Pre-pruning
importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(6,6))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()
* After Pre-Pruning, `lead_time lead time`,`market_segment_online`,`special requests`,`average_room_price` are the top important features 
#### Cost Complexity Pruning
* Cost Complexity Pruning is a post pruning method to control tree size
* we will use `DecisionTreeClassifier`
clf = DecisionTreeClassifier(random_state=1, class_weight="balanced")
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = abs(path.ccp_alphas), path.impurities
pd.DataFrame(path)
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.show()
Next, we train a decision tree using effective alphas. The last value in `ccp_alphas` is the alpha value that prunes the whole tree, leaving the tree, clfs[-1], with one node.
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(
        random_state=1, ccp_alpha=ccp_alpha, class_weight="balanced"
    )
    clf.fit(X_train, y_train) ## fit decision tree on training data
    clfs.append(clf)
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1, figsize=(8, 5))
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()
#### F1 Score vs Alpha for training sets
f1_train = []
for clf in clfs:
    pred_train = clf.predict(X_train)
    values_train = f1_score(y_train, pred_train)
    f1_train.append(values_train)

f1_test = []
for clf in clfs:
    pred_test = clf.predict(X_test)
    values_test = f1_score(y_test, pred_test)
    f1_test.append(values_test)
fig, ax = plt.subplots(figsize=(15, 5))
ax.set_xlabel("alpha")
ax.set_ylabel("F1 Score")
ax.set_title("F1 Score vs alpha for training and testing sets")
ax.plot(ccp_alphas, f1_train, marker="o", label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, f1_test, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()
index_best_model = np.argmax(f1_test)
best_model = clfs[index_best_model]
print(best_model)
#### Checking model on training set
confusion_matrix_sklearn(best_model, X_train, y_train)
decision_tree_post_perf_train = model_performance_classification_sklearn(best_model, X_train, y_train)
decision_tree_post_perf_train
#### Checking model on test data set
confusion_matrix_sklearn(best_model, X_test, y_test)
decision_tree_post_test = model_performance_classification_sklearn(best_model, X_test, y_test)
decision_tree_post_test
#Let's plot the tree plot
plt.figure(figsize=(20, 10))

out = tree.plot_tree(
    best_model,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")
        arrow.set_linewidth(1)
plt.show()
#Let's see the decision tree report
print(tree.export_text(best_model, feature_names=feature_names, show_weights=True))
#Let's plot important features 
importances = best_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()
* `lead_time` and `market_segment_type_online` are still top features in the pruned tree
## Model Performance Comparison and Conclusions
### Model performance comparison on the training data
models_train_comp_df = pd.concat(
    [
        decision_tree_perf_train.T,
        decision_tree_tune_perf_train.T,
        decision_tree_post_perf_train.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree sklearn",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post-Pruning)",
]
print("Training Model performance comparison:")
models_train_comp_df
### Model performance comparison on the testing data
models_test_comp_df = pd.concat(
    [
        decision_tree_perf_test.T,
        decision_tree_tune_perf_test.T,
        decision_tree_post_test.T,
    ],
    axis=1,
)
models_test_comp_df.columns = [
    "Decision Tree sklearn",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post-Pruning)",
]
print("Testing Model performance comparison:")
models_test_comp_df
* The parameters Accuracy,Recall,Precision and F1-score have better values in the post pruned tree than Pre-pruned tree
## Conclusion

* Usually atleast 2 person are inculded in the booking 
* People try to book the rooms for an average of 2 nights during weekdays and 1 night during weekends
* Usually booking are done around 3 months (85 days) in advance
* From the graph it is visible that room type 1 is more preferred
* Average room price is 104 Euros
* Customers making the reservations mostly first timers as number of previous cancellations is very low
* Majority guests book the room online
## Actionable Insights and Recommendations

### What profitable policies for cancellations and refunds can the hotel adopt?
* Lead time, market segment, average room price, number of special requests are the top 4 most important variables. Increasing these parameters increase the probability of the cancellations
* Hotel must display the booking cancellation at the time of booking clearly, because the online booking segment is the highest while making reservations 
* When making a reservation the hotel should make the customer aware of all the special requests, as seen from the graph the more special requests are made the cancellations are reduced
* There should some promotions (like free breakfast and Meal) and discounts during low booking months like Jan,Feb and March to lure the customers
* Also to increase the bookings during the weekend Hotel should implement the same strategy as mentioned above
* As we can observe from the graph, the higher average room price the probability of cancellations is more, Hotel should be flexible for the prices

### What other recommedations would you suggest to the hotel?
* There should be more data available for more years rather than only 2017 and 2018, that will prove helpful in predicting cancellations
* Almost no repeated guest has cancelled the booking, so the INN Hotels should have a loyalty program to attract more regular customers
* The Cancellations/Refund policies should be more flexible to attract more customers
